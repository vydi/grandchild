{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ug2C7GHUNO"
      },
      "outputs": [],
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ieA4poCNHdSY"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCGxEikHgUc"
      },
      "source": [
        "# MatFormer Lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOG0dm7LHmBE"
      },
      "source": [
        " <table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/[Gemma_3n]MatFormer_Lab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6A5O7ofZTH"
      },
      "source": [
        "Gemma 3n is a multimodal, multilingual model from the Gemma family of models. You can read about Gemma 3n in [its docs](https://ai.google.dev/gemma/docs/gemma-3n) and [launch blog post](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide). It is a unique model that is natively elastic, which means you have nested models! Gemma 3n was trained as a E4B model (effectively loads 4B parameters) with 35 layers and a 16,384 FFN hidden dimension. It has a E2B (30 layers and 8,192 FFN hidden dimension) nested within it and jointly trained as a MatFormer.\n",
        "\n",
        "[MatFormer](https://arxiv.org/abs/2310.07707) (🪆Matryoshka Transformer) architecture is a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of [Matryoshka Representation Learning](https://huggingface.co/papers/2205.13147) from just embeddings to all transformer components.\n",
        "\n",
        "Saving memory by having E2B nested within E4B is practically useful, but what makes MatFormer so powerful is its capability to smoothly span the entire Pareto-optimal accuracy-vs-model size cruve between E2B and E4B without any additional training. Using a simple technique called **Mix-n-Match** one can extract a model of any size between E2B and E4B from the main E4B model.\n",
        "\n",
        "Why would you slice a model? Given your specific deployment requirements, the E2B and E4B may not be the right fit. You may want to get an E3B, for example, which has quality higher than the E2B while requiring less compute than E4B.\n",
        "\n",
        "**In this notebook, you get to play with MatFormers and Mix-n-Match. You will specify which configuration you want for the submodel based on FFN dimension and skp layers, and then you will export the model to Hugging Face, enabling you to use with your favorite tools.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys5kfGnpHsDV"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YBXCVVERIGzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfaaa35d-7b84-483b-8e55-4bfa182a20f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.53 in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: timm>=1.0.16 in /usr/local/lib/python3.11/dist-packages (1.0.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.53) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm>=1.0.16) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm>=1.0.16) (0.21.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.53) (2025.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.53) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.53) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.53) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.53) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.53) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.53) (2025.7.14)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm>=1.0.16)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm>=1.0.16) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm>=1.0.16) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=1.0.16) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm>=1.0.16) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "# @markdown Run this cell to install all the required dependencies. In particular, you'll need Hugging Face `transformers` and `timm` versions that support Gemma 3n. Note that you may need to restart the notebook after executing the following cell.\n",
        "\n",
        "# Install a transformers version that supports Gemma 3n (>= 4.53)\n",
        "!pip install \"transformers>=4.53\" \"timm>=1.0.16\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GFh8AhruHeMl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "2b4550c2b13345d1b9b063f4e2905410",
            "658489113e774f14abcb3a9eb88d82cc",
            "9e537fb6258a437fbc187c1dcc349873",
            "4132cc4c52124959a1eb7673d022d812",
            "8a33e5b57aff49fba9fe7a90bc974147",
            "7658373aaacf4a308d82feeeb86b298e",
            "94f36b8f7b274c8c82bf14840aeda55c",
            "8d8baacfa13a4fe89a0296e4d36844a0",
            "0e2b2b6f3a9f4eae8fa61c050a4bbf92",
            "547cad0b0ead41888a3290fb9b45f829",
            "646bcb1be5fb473ca7a16d38883dff21",
            "de415e2edfb2491f98065c460d794b6c",
            "bd5ab53b7b624ef79b073cf2054c6528",
            "09e8fe78c7b948eaad2f1ba7f9bef693",
            "a6b3d31fb19b4403a1ea18f487672d0b",
            "c1d9a13e3a2f48ed90f1f8138b86eac0",
            "4e5df2441ea943e681fde517be7089a2",
            "47833644e9294a83a2cd916e768aefbf",
            "ab81655c1d25432ea792f36af915f377",
            "b4ba9440fab345a4be1fc9cf07266079"
          ]
        },
        "outputId": "2900e3fd-4c0f-4fb1-a781-5074aa96bd52"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b4550c2b13345d1b9b063f4e2905410"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Login to Hugging Face\n",
        "# @markdown This is required so you can push the model to Hugging Face. You also need to make sure you have access to the Gemma 3n model repositories.\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yKFl4n5cLF9J"
      },
      "outputs": [],
      "source": [
        "# @title Import and Export Options\n",
        "# @markdown The MatFormer Lab allows you to load a Gemma 3n 4B checkpoint (either pre-trained or instruct-tuned) and to slice it. Below, please specify:\n",
        "\n",
        "# @markdown * The original repository ID from the checkpoint in Hugging Face\n",
        "\n",
        "# @markdown * A local path where the model will be saved\n",
        "\n",
        "# @markdown * A name of a repository to push the new checkpoint to\n",
        "\n",
        "original_model_id = \"google/gemma-3n-E4B-it\" # @param [\"google/gemma-3n-E4B-it\", \"google/gemma-3n-E4B-pt\"]\n",
        "local_output_path = \"grandchild_modified_gemma_3n_model\" # @param {type:\"string\"}\n",
        "push_hf_repo_id = \"vydibalt/test-submodel\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P16x_lxGCcgc"
      },
      "source": [
        "### Slicing configuration\n",
        "\n",
        "As part of the Gemma 3n release, we share optimal slicing configurations as a [Hugging Face dataset repository](https://huggingface.co/datasets/google/gemma3n-slicing-configs), although you can also explore with your own configurations below.\n",
        "\n",
        "Each configuration specifies:\n",
        "* The hidden dimensions of the FFN\n",
        "* Which layers, if any, to skip\n",
        "* A MMLU accuracy associated with the pre-trained checkpoints\n",
        "\n",
        "`layer-level` and `block-level` configurations are a result of varying the hidden dimensions of the FFN either at a `layer-level` (fine-grained) or at a `block-level` (4 local + 1 global layers).\n",
        "\n",
        "At a `layer-level`, we found that having the global layers (instead of local layers) have higher capacity helps with accuracy for the same model size.\n",
        "\n",
        "At a `block-level`, we found that the block skipped for E2B (ie., layers 20-24) would benfit from higher capacity when not skipped and earlier blocks can work well with lower-capacity compared to the later blocks.\n",
        "\n",
        "**We invite the community to find even better configurations that lie on the Pareto-optimal curve between E2B and E4B!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UHdEebcGYats",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "976e0dee-0fa0-406e-b55d-65f4f8f21eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              name  # Layers  # Effective Params (B)  \\\n",
              "0                       Main model        35                    3.98   \n",
              "1    Config for official E2B Model        30                    1.91   \n",
              "2  Config for E1.96B (layer-level)        30                    1.96   \n",
              "3  Config for E2.54B (layer-level)        35                    2.54   \n",
              "4  Config for E2.69B (layer-level)        35                    2.69   \n",
              "\n",
              "  MMLU PT accuracy                                    FFN Hidden Dims  \\\n",
              "0           62.30%  [2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2...   \n",
              "1           50.90%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "2           53.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "3           55.40%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "4           57.70%  [2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...   \n",
              "\n",
              "         Layers Skipped  \n",
              "0                   NaN  \n",
              "1  [20, 21, 22, 23, 24]  \n",
              "2  [20, 21, 22, 23, 24]  \n",
              "3                   NaN  \n",
              "4                   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b326f4a-32b2-4f20-a643-cae314582aa9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th># Layers</th>\n",
              "      <th># Effective Params (B)</th>\n",
              "      <th>MMLU PT accuracy</th>\n",
              "      <th>FFN Hidden Dims</th>\n",
              "      <th>Layers Skipped</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Main model</td>\n",
              "      <td>35</td>\n",
              "      <td>3.98</td>\n",
              "      <td>62.30%</td>\n",
              "      <td>[2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Config for official E2B Model</td>\n",
              "      <td>30</td>\n",
              "      <td>1.91</td>\n",
              "      <td>50.90%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Config for E1.96B (layer-level)</td>\n",
              "      <td>30</td>\n",
              "      <td>1.96</td>\n",
              "      <td>53.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Config for E2.54B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.54</td>\n",
              "      <td>55.40%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Config for E2.69B (layer-level)</td>\n",
              "      <td>35</td>\n",
              "      <td>2.69</td>\n",
              "      <td>57.70%</td>\n",
              "      <td>[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b326f4a-32b2-4f20-a643-cae314582aa9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b326f4a-32b2-4f20-a643-cae314582aa9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b326f4a-32b2-4f20-a643-cae314582aa9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d1595d84-2a1d-4422-bd64-22153c163dc2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1595d84-2a1d-4422-bd64-22153c163dc2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d1595d84-2a1d-4422-bd64-22153c163dc2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"Main model\",\n          \"Config for official E2B Model\",\n          \"Config for E2.98B (layer-level)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"# Layers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 30,\n        \"max\": 35,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          30,\n          35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"# Effective Params (B)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6242168426650042,\n        \"min\": 1.91,\n        \"max\": 3.98,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          3.79,\n          2.73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MMLU PT accuracy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"54.50%\",\n          \"60.70%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FFN Hidden Dims\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"[2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8, 2_048 * 8]\",\n          \"[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Layers Skipped\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"[20, 21, 22, 23, 24]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/google/gemma3n-slicing-configs/configs.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldNQRKptGnbU"
      },
      "source": [
        "Based on your deployment scenarios, you may want to pick a different config. Select below your preferred one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfNaTzifGa9x",
        "outputId": "d3437ca1-6a42-4eba-d06e-0b90bdaa7755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config for official E2B Model\n",
            "\n",
            "Layers Skipped:\n",
            "[20, 21, 22, 23, 24]\n",
            "\n",
            "FFN Hidden Dims:\n",
            "[2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4, 2_048 * 4]\n"
          ]
        }
      ],
      "source": [
        "#@title Config details\n",
        "\n",
        "import ast\n",
        "\n",
        "config_name = \"Config for official E2B Model\"# @param ['Config for official E2B Model', 'Config for E1.96B (layer-level)', 'Config for E2.54B (layer-level)', 'Config for E2.69B (layer-level)', 'Config for E2.98B (layer-level)', 'Config for E3.18B (layer-level)', 'Config for E3.39B (layer-level)', 'Config for E3.59B (layer-level)', 'Config for E3.79B (layer-level)', 'Config for E2.49B (block-level)', 'Config for E2.73B (block-level)', 'Config for E2.98B (block-level)', 'Config for E3.24B (block-level)', 'Config for E3.49B (block-level)', 'Config for E3.79B (block-level)']\n",
        "\n",
        "def safe_string_to_list(value):\n",
        "    \"\"\"\n",
        "    Converts a string representation of a list into a Python list.\n",
        "    - Converts NaN/missing values to an empty list [].\n",
        "    - Uses eval() to handle expressions like '2_048 * 8'.\n",
        "    - Safely handles non-string values by returning them as is.\n",
        "    \"\"\"\n",
        "    # First, check if the value is missing (NaN, None, etc.)\n",
        "    if isinstance(value, list):\n",
        "        return value\n",
        "\n",
        "    # Priority 2: Now that we know it's not a list, check if it's a missing value.\n",
        "    if pd.isna(value):\n",
        "        return []\n",
        "\n",
        "    # Priority 3: If it's a string, try to evaluate it.\n",
        "    if isinstance(value, str):\n",
        "        try:\n",
        "            return eval(value)\n",
        "        except (SyntaxError, NameError):\n",
        "            return value  # Return invalid string as is\n",
        "\n",
        "    # Fallback for any other type (like an integer)\n",
        "    return value\n",
        "\n",
        "df['FFN Hidden Dims List'] = df['FFN Hidden Dims'].apply(safe_string_to_list)\n",
        "df['Layers Skipped'] = df['Layers Skipped'].apply(safe_string_to_list)\n",
        "\n",
        "df_indexed = df.set_index('name')\n",
        "model_row = df_indexed.loc[config_name]\n",
        "\n",
        "layers_to_skip = model_row['Layers Skipped']\n",
        "ffn_hidden_dims = model_row['FFN Hidden Dims List']\n",
        "ffn_hidden_dims_str = model_row['FFN Hidden Dims']\n",
        "\n",
        "print(config_name)\n",
        "print(\"\\nLayers Skipped:\")\n",
        "print(layers_to_skip)\n",
        "print(\"\\nFFN Hidden Dims:\")\n",
        "print(ffn_hidden_dims_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMY2cY6JY15G"
      },
      "source": [
        "If you want to set up your own configuration, please uncomment the following cell and assign values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4Lip1avNY9xd"
      },
      "outputs": [],
      "source": [
        "# Custom config\n",
        "layers_to_skip = [15] # e.g. [20, 21, 22, 23, 24]\n",
        "ffn_hidden_dims = [3072] # e.g. [2048 * 4, ...]\n",
        "ffn_hidden_dims_str = str(ffn_hidden_dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmZcNRV5IUmk"
      },
      "source": [
        "## Slicing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7r0gnOaIWO3"
      },
      "source": [
        "### Load the model config and verify slicing configuration\n",
        "\n",
        "Note: we do not load the model at this stage, just verify that the slicing configuration is possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O9qsekLDLPiv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer\n",
        "\n",
        "original_config = AutoConfig.from_pretrained(original_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8pgVAGEHA3w4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "45d7fd2d-8601-4343-fedf-2d6701137d97"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The length of ffn_hidden_dims (1) must be equal to the final number of layers (34).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-814855571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_hidden_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfinal_num_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;34mf\"The length of ffn_hidden_dims ({len(ffn_hidden_dims)}) must be equal \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34mf\"to the final number of layers ({final_num_layers}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The length of ffn_hidden_dims (1) must be equal to the final number of layers (34)."
          ]
        }
      ],
      "source": [
        "model_config = original_config.text_config\n",
        "\n",
        "num_layers = model_config.num_hidden_layers\n",
        "final_num_layers = num_layers - len(layers_to_skip)\n",
        "\n",
        "if len(ffn_hidden_dims) != final_num_layers:\n",
        "    raise ValueError(\n",
        "        f\"The length of ffn_hidden_dims ({len(ffn_hidden_dims)}) must be equal \"\n",
        "        f\"to the final number of layers ({final_num_layers}).\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pytVtFyqJPfp"
      },
      "source": [
        "### Update configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz8KQOEEBERT"
      },
      "outputs": [],
      "source": [
        "num_kv_comp_layers = model_config.num_hidden_layers - model_config.num_kv_shared_layers\n",
        "local_kv_sharing_layer_idx = num_kv_comp_layers - 2\n",
        "global_kv_sharing_layer_idx = num_kv_comp_layers - 1\n",
        "\n",
        "if (local_kv_sharing_layer_idx in layers_to_skip or global_kv_sharing_layer_idx in layers_to_skip):\n",
        "  raise ValueError(f'Layers {local_kv_sharing_layer_idx} and {global_kv_sharing_layer_idx} are reserved.')\n",
        "\n",
        "count_kv_sharing = sum(1 for layer in layers_to_skip if layer >= 20)\n",
        "model_config.num_kv_shared_layers -= count_kv_sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXEK9hdyBKA1"
      },
      "outputs": [],
      "source": [
        "count_activation_sparsity = sum(1 for layer in layers_to_skip if layer <= 9)\n",
        "activation_sparsity_list = [0.95] * (10 - count_activation_sparsity) + [0] * (\n",
        "    final_num_layers - 10 + count_activation_sparsity\n",
        ")\n",
        "model_config.activation_sparsity_pattern = activation_sparsity_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urMZZ2SLBOUr"
      },
      "outputs": [],
      "source": [
        "model_config.num_hidden_layers = final_num_layers\n",
        "model_config.intermediate_size = ffn_hidden_dims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRMQ97hJFtX"
      },
      "source": [
        "### Save the configuration and the unchanged tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGCI9MqoBP0w"
      },
      "outputs": [],
      "source": [
        "original_config.save_pretrained(local_output_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(original_model_id)\n",
        "tokenizer.save_pretrained(local_output_path)\n",
        "\n",
        "print(f\"New config saved to {local_output_path}\")\n",
        "print(f\"Final number of layers: {model_config.num_hidden_layers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JlVceVlJVG9"
      },
      "source": [
        "### Load the model checkpoints\n",
        "\n",
        "Note: we are saving the model to disk, so there's no need to have a large CPU/GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qh_uISmBnVR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_path = snapshot_download(original_model_id, allow_patterns=[\"*.safetensors\"])\n",
        "safetensor_files = [os.path.join(model_path, f) for f in os.listdir(model_path) if f.endswith('.safetensors')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHu0up8Jp6n"
      },
      "source": [
        "### Slice the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I2DvFSKB285"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "kept_layers_indices = [i for i in range(num_layers) if i not in layers_to_skip]\n",
        "layer_rename_map = {old_idx: new_idx for new_idx, old_idx in enumerate(kept_layers_indices)}\n",
        "\n",
        "# This will store the mapping of tensor names to the file they are saved in\n",
        "weight_map = {}\n",
        "\n",
        "# This will store tensors for the current shard we are building\n",
        "new_shard_state_dict = {}\n",
        "shard_counter = 1\n",
        "total_size = 0\n",
        "\n",
        "pbar = tqdm(total=len(safetensor_files), desc=\"Processing shards\")\n",
        "\n",
        "for shard_path in safetensor_files:\n",
        "    # Open a shard for streaming\n",
        "    with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
        "        # Iterate over each tensor in the shard\n",
        "        for tensor_name in f.keys():\n",
        "            new_tensor_name = tensor_name\n",
        "            tensor = f.get_tensor(tensor_name)\n",
        "\n",
        "            # Case 1: Handle layer-specific parameters\n",
        "            match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n",
        "            if match:\n",
        "                old_layer_idx = int(match.group(1))\n",
        "\n",
        "                # If this layer is meant to be skipped, we just continue to the next tensor\n",
        "                if old_layer_idx in layers_to_skip:\n",
        "                    continue\n",
        "\n",
        "                # Get the new sequential layer index\n",
        "                new_layer_idx = layer_rename_map[old_layer_idx]\n",
        "                new_tensor_name = tensor_name.replace(\n",
        "                    f'.layers.{old_layer_idx}.',\n",
        "                    f'.layers.{new_layer_idx}.'\n",
        "                )\n",
        "\n",
        "                # Get the target FFN dimension for this new layer\n",
        "                target_ffn_dim = ffn_hidden_dims[new_layer_idx]\n",
        "\n",
        "                # Check if this parameter is part of the FFN and needs slicing\n",
        "                if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n",
        "                    # These layers project from model_dim -> ffn_hidden_dim.\n",
        "                    # We slice the output dimension (dim 0).\n",
        "                    tensor = tensor[:target_ffn_dim, :].contiguous()\n",
        "                elif 'mlp.down_proj.weight' in new_tensor_name:\n",
        "                    # This layer projects from ffn_hidden_dim -> model_dim.\n",
        "                    # We slice the input dimension (dim 1).\n",
        "                    tensor = tensor[:, :target_ffn_dim].contiguous()\n",
        "\n",
        "            # Case 2: Handle special non-layer parameters that need slicing\n",
        "            elif 'per_layer_model_projection' in tensor_name:\n",
        "                # Reshape, slice based on kept layers, and reshape back\n",
        "                reshaped_params = tensor.reshape((num_layers, tensor.shape[0] // num_layers, tensor.shape[1]))\n",
        "                tensor = reshaped_params[kept_layers_indices, :, :]\n",
        "                tensor = tensor.reshape(-1, tensor.shape[-1]).contiguous()\n",
        "\n",
        "            elif 'embed_tokens_per_layer' in tensor_name:\n",
        "                # Reshape, slice based on kept layers, and reshape back\n",
        "                reshaped_params = tensor.reshape((tensor.shape[0], num_layers, tensor.shape[1] // num_layers))\n",
        "                tensor = reshaped_params[:, kept_layers_indices, :]\n",
        "                tensor = tensor.reshape(tensor.shape[0], -1).contiguous()\n",
        "\n",
        "            # Add the (potentially modified) tensor to the new shard\n",
        "            new_shard_state_dict[new_tensor_name] = tensor\n",
        "\n",
        "            # Check if the current shard is getting too big\n",
        "            current_shard_size = sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())\n",
        "            if current_shard_size > 4000000000: # Create new shard if current is over 4GB\n",
        "                shard_filename = f\"model-{(shard_counter):05d}-of-XXXXX.safetensors\"\n",
        "                print(f\"Saving shard {shard_filename} (size: {current_shard_size / 1e9:.2f} GB)\")\n",
        "                save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename), metadata={'format': 'pt'})\n",
        "\n",
        "                # Record which tensors are in this shard\n",
        "                for k in new_shard_state_dict.keys():\n",
        "                    weight_map[k] = os.path.basename(shard_filename)\n",
        "\n",
        "                # Reset for the next shard\n",
        "                shard_counter += 1\n",
        "                new_shard_state_dict = {}\n",
        "                gc.collect() # Free up memory\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgN1yAQ-CFkE"
      },
      "outputs": [],
      "source": [
        "# Save any remaining tensors in the last shard\n",
        "if new_shard_state_dict:\n",
        "    shard_filename = f\"model-{(shard_counter):05d}-of-XXXXX.safetensors\"\n",
        "    print(f\"Saving final shard {shard_filename}\")\n",
        "    save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename), metadata={'format': 'pt'})\n",
        "    for k in new_shard_state_dict.keys():\n",
        "        weight_map[k] = os.path.basename(shard_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZuSMTulGUPv"
      },
      "outputs": [],
      "source": [
        "del new_shard_state_dict\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcEWpUKOGO4A"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "print(\"\\n--- 3. Finalizing Model Save ---\")\n",
        "\n",
        "# The total number of shards we created\n",
        "num_shards = shard_counter\n",
        "\n",
        "# Update the \"XXXXX\" in the filenames to the correct total number of shards\n",
        "for i in range(1, num_shards + 1):\n",
        "    old_filename = f\"model-{(i):05d}-of-XXXXX.safetensors\"\n",
        "    new_filename = f\"model-{(i):05d}-of-{(num_shards):05d}.safetensors\"\n",
        "\n",
        "    # Rename the file\n",
        "    os.rename(os.path.join(local_output_path, old_filename), os.path.join(local_output_path, new_filename))\n",
        "\n",
        "    # Update the weight_map to point to the new filename\n",
        "    for k, v in weight_map.items():\n",
        "        if v == old_filename:\n",
        "            weight_map[k] = new_filename\n",
        "\n",
        "# Create and save the index.json file\n",
        "index_json = {\n",
        "    \"metadata\": {\n",
        "        \"total_size\": sum(os.path.getsize(os.path.join(local_output_path, f)) for f in os.listdir(local_output_path) if f.endswith('.safetensors'))\n",
        "    },\n",
        "    \"weight_map\": weight_map\n",
        "}\n",
        "\n",
        "with open(os.path.join(local_output_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
        "    json.dump(index_json, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Model slicing complete. New model saved in: {local_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1khD_77N8YX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import  ModelCard, ModelCardData\n",
        "\n",
        "card = ModelCard.load(original_model_id)\n",
        "card.data.base_model = original_model_id\n",
        "del card.data.extra_gated_heading\n",
        "del card.data.extra_gated_prompt\n",
        "card.data.tags.append(\"matformer\")\n",
        "\n",
        "new_description = f\"\"\"\n",
        "> [!Note]\n",
        "> This is a submodel derived from `{original_model_id}`. It has been modified by slicing specific layers and resizing FFN dimensions. It is not the original model.\n",
        "> To learn more about MatFormers, please review the [launch blog](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide) and generate your own submodels\n",
        "with the [MatFormer Lab](https://goo.gle/gemma3n-matformer-lab).\n",
        ">\n",
        "\n",
        "Skipped layers: {layers_to_skip}\n",
        "\n",
        "FFN hidden dimensions: {ffn_hidden_dims_str}\n",
        "\"\"\"\n",
        "\n",
        "card.text = new_description + \"\\n\" + card.text\n",
        "print(\"Prepended custom description to the model card content.\")\n",
        "\n",
        "new_readme_path = os.path.join(local_output_path, \"README.md\")\n",
        "card.save(new_readme_path)\n",
        "print(f\"New README.md saved to '{new_readme_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuF3U6nhOFwu"
      },
      "source": [
        "## Push the model to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hamNb9aGkjP"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "print(f\"Creating private repository: {push_hf_repo_id}\")\n",
        "\n",
        "# Instantiate the HfApi client\n",
        "api = HfApi()\n",
        "\n",
        "# Create a new private repository on the Hub.\n",
        "repo_url = api.create_repo(\n",
        "    repo_id=push_hf_repo_id,\n",
        "    private=True,\n",
        "    exist_ok=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OEIMgDQHCUM"
      },
      "outputs": [],
      "source": [
        "print(f\"Uploading files from '{local_output_path}' to '{push_hf_repo_id}'...\")\n",
        "api.upload_folder(\n",
        "    folder_path=local_output_path,\n",
        "    repo_id=push_hf_repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload sliced model checkpoint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Gf-3Tg2TaJs",
        "colab": {
          "referenced_widgets": [
            "db433ba38c874916a76d23eb45ff0ac2"
          ]
        },
        "outputId": "a2ec3770-0bea-4cc1-ccdb-75aab5c0c0d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db433ba38c874916a76d23eb45ff0ac2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameters: 5,976,833,408\n",
            "Total Text Parameters: 4,456,156,768\n",
            "Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): 1,905,495,648\n"
          ]
        }
      ],
      "source": [
        "#@title Verify new model can be loaded\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(push_hf_repo_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "print(f\"Total Parameters: {model.num_parameters():,}\") # 5,976,833,408\n",
        "print(f\"Total Text Parameters: {model.language_model.num_parameters():,}\") # 4,456,156,768\n",
        "print(f\"Effective Parameters (excluding vision, audio, and Per-Layer-Embeddings): {model.language_model.num_parameters(exclude_embeddings=True):,}\") # 1,905,495,648"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO76mJDtOAr7"
      },
      "source": [
        "## Citation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGDUTwIkKrnp"
      },
      "source": [
        "```\n",
        "@article{gemma_3n_2025,\n",
        "    title={Gemma 3n MatFormer Lab},\n",
        "    url={https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/%5BGemma_3n%5DMatFormer_Lab.ipynb},\n",
        "    publisher={Google DeepMind},\n",
        "    author={Puranjay Datta, Aditya Kusupati, Ryan Mullins, Omar Sanseviero, Rakesh Shivanna, Gemma Team},\n",
        "    year={2025}\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "B7r0gnOaIWO3",
        "pytVtFyqJPfp",
        "pjRMQ97hJFtX",
        "1JlVceVlJVG9",
        "3RHu0up8Jp6n"
      ],
      "name": "[Gemma_3n]MatFormer_Lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b4550c2b13345d1b9b063f4e2905410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_94f36b8f7b274c8c82bf14840aeda55c"
          }
        },
        "658489113e774f14abcb3a9eb88d82cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d8baacfa13a4fe89a0296e4d36844a0",
            "placeholder": "​",
            "style": "IPY_MODEL_0e2b2b6f3a9f4eae8fa61c050a4bbf92",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "9e537fb6258a437fbc187c1dcc349873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_547cad0b0ead41888a3290fb9b45f829",
            "placeholder": "​",
            "style": "IPY_MODEL_646bcb1be5fb473ca7a16d38883dff21",
            "value": ""
          }
        },
        "4132cc4c52124959a1eb7673d022d812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_de415e2edfb2491f98065c460d794b6c",
            "style": "IPY_MODEL_bd5ab53b7b624ef79b073cf2054c6528",
            "value": true
          }
        },
        "8a33e5b57aff49fba9fe7a90bc974147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_09e8fe78c7b948eaad2f1ba7f9bef693",
            "style": "IPY_MODEL_a6b3d31fb19b4403a1ea18f487672d0b",
            "tooltip": ""
          }
        },
        "7658373aaacf4a308d82feeeb86b298e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1d9a13e3a2f48ed90f1f8138b86eac0",
            "placeholder": "​",
            "style": "IPY_MODEL_4e5df2441ea943e681fde517be7089a2",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "94f36b8f7b274c8c82bf14840aeda55c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8d8baacfa13a4fe89a0296e4d36844a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2b2b6f3a9f4eae8fa61c050a4bbf92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "547cad0b0ead41888a3290fb9b45f829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646bcb1be5fb473ca7a16d38883dff21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de415e2edfb2491f98065c460d794b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd5ab53b7b624ef79b073cf2054c6528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09e8fe78c7b948eaad2f1ba7f9bef693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b3d31fb19b4403a1ea18f487672d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c1d9a13e3a2f48ed90f1f8138b86eac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e5df2441ea943e681fde517be7089a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47833644e9294a83a2cd916e768aefbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab81655c1d25432ea792f36af915f377",
            "placeholder": "​",
            "style": "IPY_MODEL_b4ba9440fab345a4be1fc9cf07266079",
            "value": "Connecting..."
          }
        },
        "ab81655c1d25432ea792f36af915f377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4ba9440fab345a4be1fc9cf07266079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}